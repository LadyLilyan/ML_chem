{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73a72cab",
      "metadata": {
        "id": "73a72cab"
      },
      "source": [
        "# Getting Started with PyTorch\n",
        "    \n",
        "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
        "\n",
        "**Authors**:\n",
        "- Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
        "- Katerina Karoni\n",
        "\n",
        "Some content was also adapted from the [scikit-learn](https://scikit-learn.org/stable/auto_examples/index.html) and [pytorch](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) documentation and examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96047d0",
      "metadata": {
        "id": "c96047d0"
      },
      "source": [
        "**Learning Objectives**:\n",
        "* Getting familiar with some PyTorch basics:\n",
        "    * What is a tensor\n",
        "    * How to build a dataset\n",
        "    * How to build a neural network\n",
        "    * Choosing an optimiser\n",
        "    * Testing and training a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33497e6",
      "metadata": {
        "id": "f33497e6"
      },
      "source": [
        "**Jupyter cheat sheet**:\n",
        "* to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
        "* to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e97feb8",
      "metadata": {
        "id": "2e97feb8"
      },
      "source": [
        "## PyTorch Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba58247d",
      "metadata": {
        "id": "ba58247d"
      },
      "source": [
        "PyTorch is a Python framework that provides two high-level features:\n",
        "\n",
        "    Tensor computation (like NumPy) with strong GPU acceleration\n",
        "    Deep neural networks built on a autograd system\n",
        "\n",
        "You can reuse your favorite Python packages such as NumPy to extend PyTorch when needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5215a1b",
      "metadata": {
        "id": "b5215a1b"
      },
      "source": [
        "**<h2>Pytorch Installation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0317ce5",
      "metadata": {
        "id": "c0317ce5"
      },
      "source": [
        "So far we have only used scikit-learn. Today we will be using PyTorch. If you wanted to install it locally then use the command below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46f601b",
      "metadata": {
        "id": "c46f601b"
      },
      "source": [
        "```conda install pytorch torchvision -c pytorch```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dff5d68",
      "metadata": {
        "id": "6dff5d68"
      },
      "source": [
        "For more information on the installation please check out: https://pytorch.org/get-started/locally/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22931231",
      "metadata": {
        "id": "22931231"
      },
      "source": [
        "⚠️ If you want to make use of the GPUs on Colab we should use the Colab version of today's notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4cb3391",
      "metadata": {
        "id": "c4cb3391"
      },
      "source": [
        "## Google Colab package installs\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "The following cell install necessary packages and downloads data if you are running this tutorial using Google Colab.<br>\n",
        "<b><i>Run this cell only if you are using Google Colab!</i></b></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bab3c3e6-3556-47ee-b299-4fa31aa736fb",
      "metadata": {
        "id": "bab3c3e6-3556-47ee-b299-4fa31aa736fb",
        "outputId": "ed73ba14-a2c0-42b3-8ca6-beb4d0db7aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ATCP-ML-workshop'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 164 (delta 24), reused 26 (delta 8), pack-reused 114 (from 1)\u001b[K\n",
            "Receiving objects: 100% (164/164), 109.19 MiB | 9.65 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "Updating files: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then git clone https://github.com/Edinburgh-Chemistry-Teaching/ATCP-ML-workshop; fi\n",
        "import os\n",
        "os.chdir(f\"ATCP-ML-workshop{os.sep}Workshops{os.sep}workshop_03\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc9bed8",
      "metadata": {
        "id": "ecc9bed8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cf7db7eb",
      "metadata": {
        "id": "cf7db7eb"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53553e38",
      "metadata": {
        "id": "53553e38"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either **CPU** or **GPU**. To run operations on the GPU, just cast the Tensor to a `cuda datatype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56430451",
      "metadata": {
        "id": "56430451",
        "outputId": "7e3b9eab-701a-40b3-f05d-e5ddc12bcab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "Size of x = torch.Size([2, 3])\n",
            "Data type of x = torch.float32\n"
          ]
        }
      ],
      "source": [
        "x = torch.Tensor([[1,2,3],[4,5,6]])\n",
        "\n",
        "print(f'x = {x}')\n",
        "\n",
        "print(f'Size of x = {x.size()}') # np.shape(x) also works\n",
        "\n",
        "print(f'Data type of x = {x.dtype}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "56ea112e",
      "metadata": {
        "id": "56ea112e",
        "outputId": "8074204d-22fc-4b00-c25f-d9b681d4ba54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Data type of y = torch.float32\n"
          ]
        }
      ],
      "source": [
        "# we can also specify data type\n",
        "\n",
        "y = torch.zeros(2,3,dtype=torch.float32)\n",
        "\n",
        "print(f'y = {y}')\n",
        "\n",
        "print(f'Data type of y = {y.dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d271cf66",
      "metadata": {
        "id": "d271cf66"
      },
      "source": [
        "**Casting tensor x as CUDA datatype if CUDA available**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "06c07389",
      "metadata": {
        "id": "06c07389"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42b80cf",
      "metadata": {
        "id": "c42b80cf"
      },
      "source": [
        "Attention: ```x.to(device)``` will not cast ```x```as cuda datatype - we need\n",
        "```x = x.to(device)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "100e3d99",
      "metadata": {
        "scrolled": true,
        "id": "100e3d99",
        "outputId": "05b3abbf-9524-42e2-abd4-289b6f023cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "y = x.to(device) # or x = x.cuda() for GPU\n",
        "print(x.is_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee1ebc0",
      "metadata": {
        "id": "4ee1ebc0"
      },
      "source": [
        "Note: For tensors ```x.to(device)```, as mentioned does not move ```x``` to cuda and we need to write ```x = x.to(device)``` instead.\n",
        "\n",
        "However, for neural networks, ```net.to(device)``` and\n",
        "```net = net.to(device)``` are equivalent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6f5254",
      "metadata": {
        "id": "2f6f5254"
      },
      "source": [
        "**Tensor Data types**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad495d27",
      "metadata": {
        "id": "ad495d27"
      },
      "source": [
        "```DoubleTensor``` is ```64-bit``` floating point and ```FloatTensor``` is ```32-bit``` floating point tensor. So a ```FloatTensor``` uses half of the memory as a same tensor-size ```DoubleTensor``` uses. Also GPU and CPU computations with lower precision are much faster.  However, if high precision is needed, go for ```DoubleTensor``` . So Pytorch leaves it to user to choose which one to use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd5458a",
      "metadata": {
        "id": "3dd5458a"
      },
      "source": [
        "Set default tensor type for your notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "009acf51",
      "metadata": {
        "id": "009acf51"
      },
      "outputs": [],
      "source": [
        "#torch.set_default_dtype(torch.float32)    # 32 bits\n",
        "#or\n",
        "torch.set_default_dtype(torch.float64)    # 64 bits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1576f635",
      "metadata": {
        "id": "1576f635",
        "outputId": "f7b4c1dc-707a-4623-a13c-4a0f7f4dafbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "torch.float64\n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros(2,3)\n",
        "print(x)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037decaa",
      "metadata": {
        "id": "037decaa"
      },
      "source": [
        "## Show case: Components needed to run a classification with a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c4bfe2",
      "metadata": {
        "id": "63c4bfe2"
      },
      "source": [
        "We will now present an example where we train a fully-connected neural network on a popular benchmark dataset: MNIST handwritten digits.\n",
        "\n",
        "The MNIST had-written digits dataset consists of 60,000 training examples and 10,000 test examples. Each example comprises a 8×8 image and an associated label from one of 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b37a757d",
      "metadata": {
        "id": "b37a757d"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from sklearn import datasets\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a413d6d9",
      "metadata": {
        "id": "a413d6d9"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557735be",
      "metadata": {
        "id": "557735be"
      },
      "source": [
        "The ```torchvision``` library consists of popular datasets, model architectures, and common image transformations for computer vision. Since we have been using `scikit learn` we will use this data set in the example here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "059ffd73",
      "metadata": {
        "id": "059ffd73"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f7020ad9",
      "metadata": {
        "id": "f7020ad9",
        "outputId": "2f443994-abb3-4562-9308-12399acd8c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAADSCAYAAAAi0d0oAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEhNJREFUeJzt3W+QlWX9P/DPCrEbAbIi5JIlLDpjySABzSQm4LgQpMYqgT5gZB0bqGSM/swsU5gLlknajBVmxBMMzFFKIZtMYXBzmp7Eyloaziyx6GQ4KS5/FPnr/XvQ1/1FS+6C1+Vhd1+vGWbc65z7fV+H5eOe99xnzykriqIIAACAxM4o9QYAAICeSdkAAACyUDYAAIAslA0AACALZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZOAV1dXUxYsSIUzq2oaEhysrK0m4ITkPmBDpnTqBz5qR761Flo6ysrEt/GhsbS73V086f/vSn+MxnPhP9+/ePc845J2655ZZ44403Sr0tMjAnp+bJJ5+Mm266KUaPHh19+vQ55R98dA/m5OQdOHAg7r333pg2bVpUVVXFwIED45Of/GTcd999cezYsVJvjwzMyam544474tOf/nQMHTo0Kioq4oILLohFixbFq6++WuqtZVFWFEVR6k2ksnbt2uO+/sUvfhEbN26MNWvWHLc+derU+PCHP3zK5zly5Ei8/fbbUV5eftLHHj16NI4ePRoVFRWnfP7Umpub45JLLomPf/zjMX/+/PjHP/4Rd999d1x++eXx+OOPl3p7JGZOTk1dXV089NBDMW7cuHjppZeiT58+sXPnzlJvi0zMycl77rnnYsyYMXHFFVfEtGnTYtCgQfHEE0/Eo48+GjfccEPcf//9pd4iiZmTUzNr1qwYOnRoXHjhhTFw4MDYtm1brFq1KoYNGxbNzc3xoQ99qNRbTKvowW6++eaiKw/xzTfffB92c/qaMWNGUVVVVezdu7d9bdWqVUVEFE888UQJd8b7wZx0zcsvv1wcPny4KIqiuPLKK4vzzjuvtBvifWVOOvfqq68Wzz33XIf1G2+8sYiIoqWlpQS74v1kTk7dr371qyIiigcffLDUW0muR72MqiumTJkSo0ePjqamppg0aVL0798/vvWtb0VExIYNG+LKK6+M4cOHR3l5eYwaNSpuv/32Dpd///u1gzt37oyysrK4++674+c//3mMGjUqysvL41Of+lT8+c9/Pu7YE712sKysLBYuXBjr16+P0aNHR3l5eVx00UXx+9//vsP+GxsbY8KECVFRURGjRo2KlStXnjDztddeixdeeCEOHDjwrn8f+/bti40bN8bcuXNj0KBB7es33HBDDBgwIB5++OF3PZ6eyZx0NHz48PjABz7Q6f3oPczJ8c4+++y46KKLOqxfc801ERGxbdu2dz2ensmcdM07j2/Pnj2ndPzprG+pN1AKu3fvjhkzZsT1118fc+fObb+0t3r16hgwYEB8/etfjwEDBsTmzZvjO9/5Tuzbty/uuuuuTnN/+ctfxv79+2PBggVRVlYWP/jBD+Laa6+NHTt2dPok5Y9//GM88sgj8ZWvfCUGDhwYP/7xj2PWrFnx0ksvxZAhQyIiYuvWrTF9+vSoqqqKpUuXxrFjx2LZsmUxdOjQDnkrVqyIpUuXxlNPPRVTpkz5n+f961//GkePHo0JEyYct96vX78YO3ZsbN26tdPHTc9kTqBz5qRzr7zySkT8u4zQO5mTjoqiiN27d8fRo0ejpaUlFi9eHH369OmZP4tKfWklpxNdzps8eXIREcXPfvazDvc/cOBAh7UFCxYU/fv3Lw4ePNi+Nm/evONeQtHa2lpERDFkyJDi9ddfb1/fsGFDERHFY4891r522223ddhTRBT9+vUrtm/f3r727LPPFhFR/OQnP2lfu/rqq4v+/fsXL7/8cvtaS0tL0bdv3w6Z75znqaee6vCY/tO6deuKiCiefvrpDrfNnj27OOecc971eLo/c9L5nPw3L6PqfczJyc9JURTFoUOHik984hPFyJEjiyNHjpz08XQv5qTrc7Jr164iItr/nHvuucVDDz3UpWO7m173MqqIiPLy8rjxxhs7rH/wgx9s/+/9+/fHa6+9FpdddlkcOHAgXnjhhU5zr7vuuqisrGz/+rLLLouIiB07dnR6bE1NTYwaNar96zFjxsSgQYPajz127Fhs2rQpamtrY/jw4e33O//882PGjBkd8hoaGqIoik4b8ltvvRURccJfuqqoqGi/nd7HnEDnzMm7W7hwYfztb3+LFStWRN++vfLFFIQ5OZGzzjorNm7cGI899lgsW7Yszj777B77LqC9cvI/8pGPRL9+/TqsP//887FkyZLYvHlz7Nu377jb9u7d22nuxz72seO+fmcA2traTvrYd45/59h//etf8dZbb8X555/f4X4nWuuqdwb90KFDHW47ePDgcf8joHcxJ9A5c/K/3XXXXbFq1aq4/fbb43Of+1yyXLofc9JRv379oqamJiIirrrqqrjiiivi0ksvjWHDhsVVV131nvNPJ72ybJzoCfSePXti8uTJMWjQoFi2bFmMGjUqKioq4plnnon6+vp4++23O83t06fPCdeLLry78Hs59r2oqqqKiIhdu3Z1uG3Xrl3HtXl6F3MCnTMnJ7Z69eqor6+PL33pS7FkyZL37bycnsxJ5yZOnBhVVVXxwAMPKBs9VWNjY+zevTseeeSRmDRpUvt6a2trCXf1/w0bNiwqKipi+/btHW470VpXjR49Ovr27RtbtmyJOXPmtK8fPnw4mpubj1uD3joncDJ6+5xs2LAhvvjFL8a1114b995773vOo2fq7XNyIgcPHuzSFZ3uplf+zsaJvNNw/7PRHj58OH7605+WakvH6dOnT9TU1MT69evjn//8Z/v69u3bT/jBe119C7YzzzwzampqYu3atbF///729TVr1sQbb7wRs2fPTvcg6PZ665zAyejNc/L000/H9ddfH5MmTYoHHnggzjjD0wxOrLfOyZtvvnnC+/z617+Otra2Du8O2hO4svF/Jk6cGJWVlTFv3ry45ZZboqysLNasWXNavTyjoaEhnnzyybj00kvjy1/+chw7dixWrFgRo0ePjubm5uPuezJvwfa9730vJk6cGJMnT27/BPEf/vCHMW3atJg+fXq+B0S305vn5C9/+Uv85je/iYh//7DZu3dvfPe7342IiIsvvjiuvvrqHA+Hbqi3zsmLL74Yn//856OsrCy+8IUvxLp16467fcyYMTFmzJgMj4buqLfOSUtLS9TU1MR1110XF154YZxxxhmxZcuWWLt2bYwYMSK++tWv5n1QJaBs/J8hQ4bEb3/72/jGN74RS5YsicrKypg7d25cccUV8dnPfrbU24uIiPHjx8fjjz8e3/zmN+PWW2+Nj370o7Fs2bLYtm1bl9614X8ZN25cbNq0Kerr6+NrX/taDBw4MG666ab4/ve/n3D39AS9eU6eeeaZuPXWW49be+frefPmKRu0661z0tra2v4SkJtvvrnD7bfddpuyQbveOifnnntuzJo1KzZv3hz3339/HDlyJM4777xYuHBhfPvb327/jI+epKw4nSokp6S2tjaef/75aGlpKfVW4LRlTqBz5gQ6Z05OjhdTdjP//bkXLS0t8bvf/c7nBMB/MCfQOXMCnTMn750rG91MVVVV1NXVRXV1dbz44otx3333xaFDh2Lr1q1xwQUXlHp7cFowJ9A5cwKdMyfvnd/Z6GamT58eDz74YLzyyitRXl4el1xySdxxxx3+wcN/MCfQOXMCnTMn750rGwAAQBZ+ZwMAAMhC2QAAALJQNgAAgCx63C+I//cnlqZQX1+fPHPq1KnJMyMi7rzzzuSZlZWVyTPpeXK8DeCePXuSZ0b8+1NhU6utrU2eSc/T2NiYPDPXv72xY8cmz8zx+Cm95cuXJ89cvHhx8syRI0cmz4yIaGpqSp7Zk557ubIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZKFsAAAAWSgbAABAFsoGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBZ9S72B1Orr65Nntra2Js9sa2tLnhkRcdZZZyXPfPjhh5Nnzp49O3kmpTV48ODkmX/4wx+SZ0ZENDY2Js+sra1NnklpNTc3J8+8/PLLk2eeeeaZyTMjInbu3Jkll9JavHhx8swczxNWrlyZPHPBggXJMyMimpqakmfW1NQkzywVVzYAAIAslA0AACALZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAslA2AACALJQNAAAgC2UDAADIQtkAAACyUDYAAIAslA0AACALZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAsuhbypM3NTUlz2xtbU2e+fe//z15ZnV1dfLMiIipU6cmz8zxfZo9e3byTLquubk5eWZjY2PyzFzGjh1b6i3QDaxfvz555sUXX5w8s7a2NnlmRMTSpUuz5FJa8+fPT55ZX1+fPHP8+PHJM0eOHJk8MyKipqYmS25P4coGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZKFsAAAAWSgbAABAFsoGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBZ9S3nytra25Jnjxo1LnlldXZ08M5fx48eXegskds899yTPbGhoSJ65d+/e5Jm5TJkypdRboBtYtGhR8swRI0Ykz8yxz4iImTNnZsmltHI8p9mxY0fyzNbW1uSZNTU1yTMj8jyfraysTJ5ZKq5sAAAAWSgbAABAFsoGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZKFsAAAAWSgbAABAFsoGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGTRt5Qnb2trS545derU5JndSY6/08rKyuSZdN2iRYuSZ9bV1SXP7E7/Tvbs2VPqLZBYju/pPffckzxz/fr1yTNzWb16dam3QDdRXV2dPPP1119PnllTU5M8M1fupk2bkmeW6ue0KxsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZKFsAAAAWSgbAABAFsoGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZKFsAAAAWSgbAABAFn1LefLKysrkmU1NTckzc2hra8uSu2XLluSZc+bMSZ4JpdTc3Jw8c+zYsckz6bqGhobkmT/60Y+SZ+bw6KOPZskdPHhwllzoihzPETdt2pQ8MyJiwYIFyTOXL1+ePPPOO+9MntkVrmwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZKFsAAAAWSgbAABAFsoGAACQhbIBAABkoWwAAABZKBsAAEAWygYAAJCFsgEAAGShbAAAAFkoGwAAQBbKBgAAkIWyAQAAZNG3lCevrq5Onrlly5bkmevWresWmbnU19eXegsA76quri55ZmNjY/LMZ599NnnmNddckzwzImLmzJnJM3N8n2pra5NncnIWL16cPLOmpiZ5ZltbW/LMiIiNGzcmz5wzZ07yzFJxZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAslA2AACALJQNAAAgC2UDAADIQtkAAACyUDYAAIAslA0AACALZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAslA2AACALJQNAAAgi76lPHl1dXXyzOXLlyfPrK+vT545YcKE5JkREU1NTVly6VkGDx6cPHPmzJnJMzds2JA8MyKisbExeWZdXV3yTLpu7NixyTObm5u7RWZDQ0PyzIg88zdixIjkmbW1tckzOTmVlZXJM+fPn588M5c5c+Ykz1y5cmXyzFJxZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAslA2AACALJQNAAAgC2UDAADIQtkAAACyUDYAAIAslA0AACALZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAslA2AACALJQNAAAgi7KiKIpSbwIAAOh5XNkAAACyUDYAAIAslA0AACALZQMAAMhC2QAAALJQNgAAgCyUDQAAIAtlAwAAyELZAAAAsvh/SwTEIK6TZWQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "digits = datasets.load_digits()\n",
        "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    ax.set_title(\"Training: %i\" % label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "12249f62",
      "metadata": {
        "id": "12249f62",
        "outputId": "adc3c57c-5525-4c84-f924-5205357affa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
              "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
              " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
              " 'frame': None,\n",
              " 'feature_names': ['pixel_0_0',\n",
              "  'pixel_0_1',\n",
              "  'pixel_0_2',\n",
              "  'pixel_0_3',\n",
              "  'pixel_0_4',\n",
              "  'pixel_0_5',\n",
              "  'pixel_0_6',\n",
              "  'pixel_0_7',\n",
              "  'pixel_1_0',\n",
              "  'pixel_1_1',\n",
              "  'pixel_1_2',\n",
              "  'pixel_1_3',\n",
              "  'pixel_1_4',\n",
              "  'pixel_1_5',\n",
              "  'pixel_1_6',\n",
              "  'pixel_1_7',\n",
              "  'pixel_2_0',\n",
              "  'pixel_2_1',\n",
              "  'pixel_2_2',\n",
              "  'pixel_2_3',\n",
              "  'pixel_2_4',\n",
              "  'pixel_2_5',\n",
              "  'pixel_2_6',\n",
              "  'pixel_2_7',\n",
              "  'pixel_3_0',\n",
              "  'pixel_3_1',\n",
              "  'pixel_3_2',\n",
              "  'pixel_3_3',\n",
              "  'pixel_3_4',\n",
              "  'pixel_3_5',\n",
              "  'pixel_3_6',\n",
              "  'pixel_3_7',\n",
              "  'pixel_4_0',\n",
              "  'pixel_4_1',\n",
              "  'pixel_4_2',\n",
              "  'pixel_4_3',\n",
              "  'pixel_4_4',\n",
              "  'pixel_4_5',\n",
              "  'pixel_4_6',\n",
              "  'pixel_4_7',\n",
              "  'pixel_5_0',\n",
              "  'pixel_5_1',\n",
              "  'pixel_5_2',\n",
              "  'pixel_5_3',\n",
              "  'pixel_5_4',\n",
              "  'pixel_5_5',\n",
              "  'pixel_5_6',\n",
              "  'pixel_5_7',\n",
              "  'pixel_6_0',\n",
              "  'pixel_6_1',\n",
              "  'pixel_6_2',\n",
              "  'pixel_6_3',\n",
              "  'pixel_6_4',\n",
              "  'pixel_6_5',\n",
              "  'pixel_6_6',\n",
              "  'pixel_6_7',\n",
              "  'pixel_7_0',\n",
              "  'pixel_7_1',\n",
              "  'pixel_7_2',\n",
              "  'pixel_7_3',\n",
              "  'pixel_7_4',\n",
              "  'pixel_7_5',\n",
              "  'pixel_7_6',\n",
              "  'pixel_7_7'],\n",
              " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
              " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
              "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
              "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
              "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
              "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
              "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
              "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
              "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
              "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
              "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
              "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
              "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
              "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
              "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
              "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
              "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
              "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
              "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
              "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
              " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 1797\\n:Number of Attributes: 64\\n:Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n:Missing Attribute Values: None\\n:Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n:Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. dropdown:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "06ae0d75",
      "metadata": {
        "id": "06ae0d75"
      },
      "outputs": [],
      "source": [
        "# creating a torch tensor:\n",
        "\n",
        "x = torch.Tensor(digits['data']) # data\n",
        "y = torch.Tensor(digits['target']) # labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "11d47053",
      "metadata": {
        "id": "11d47053",
        "outputId": "2dae141a-3ca3-4a60-e890-cf1a7e9758e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2.,  ..., 8., 9., 8.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9ecb35",
      "metadata": {
        "id": "ff9ecb35"
      },
      "source": [
        "**<h3> Dataloader class**\n",
        "    \n",
        "https://pytorch.org/docs/stable/data.html#map-style-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f955aa",
      "metadata": {
        "id": "94f955aa"
      },
      "source": [
        "At the heart of ```PyTorch``` data loading utility is the ```torch.utils.data.DataLoader``` class. It represents a Python iterable over a dataset\n",
        "\n",
        "```python\n",
        "DataLoader(dataset, batch_size=1, shuffle=False,\n",
        "           sampler=None,batch_sampler=None, num_workers=0,\n",
        "           collate_fn=None,pin_memory=False, drop_last=False,\n",
        "           timeout=0, worker_init_fn=None, *, prefetch_factor=2,\n",
        "           persistent_workers=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d96636f",
      "metadata": {
        "id": "8d96636f"
      },
      "source": [
        "The most important argument of ```DataLoader``` constructor is ```dataset```, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:\n",
        "\n",
        "- Map style datasets: Datasets that implement the ```__getitem__()``` and ```__len__()``` methods and are maps from keys to data samples.\n",
        "\n",
        "- Iterable style datasets: When reading from a stream of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "33f3f386",
      "metadata": {
        "id": "33f3f386"
      },
      "outputs": [],
      "source": [
        "## Splitting the data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "# Split the train data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60779ddb",
      "metadata": {
        "id": "60779ddb"
      },
      "source": [
        "Let's check that our data is of torch data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c3d9dcfe",
      "metadata": {
        "id": "c3d9dcfe",
        "outputId": "7498358e-e079-4582-ee8c-8df422c6b9af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9830a69d",
      "metadata": {
        "id": "9830a69d"
      },
      "outputs": [],
      "source": [
        "# Next the data needs to be wrapped into a Tensor dataset and then we can use a dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "efeccfd4",
      "metadata": {
        "id": "efeccfd4"
      },
      "outputs": [],
      "source": [
        "# Wrap your data in TensorDataset\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Create the dataloaders\n",
        "batch_size = 128\n",
        "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "47acbdc7",
      "metadata": {
        "id": "47acbdc7"
      },
      "outputs": [],
      "source": [
        "# Reshape train and test images from 8x8 to 1x64\n",
        "for i,data in enumerate(dataloader_train):\n",
        "    xtrain = data[0].view(-1,64)       # torch.Tensor.view() is equivalent to reshape\n",
        "    ytrain = data[1].type(torch.LongTensor)                    # train labels\n",
        "\n",
        "for i,data in enumerate(dataloader_test):\n",
        "    xtest = data[0].view(-1,64)     # test images, The size -1 is inferred from other dimensions\n",
        "    ytest = data[1].type(torch.LongTensor)                            # test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "45b654ec",
      "metadata": {
        "id": "45b654ec"
      },
      "outputs": [],
      "source": [
        "flat_mnist_train       = TensorDataset(xtrain, ytrain)\n",
        "flat_dataloader_train  = DataLoader(flat_mnist_train, batch_size=256, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "af0faaaf",
      "metadata": {
        "id": "af0faaaf",
        "outputId": "794d718e-3354-4239-8702-1343e627a6fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([125, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "np.shape(xtrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c42b6143",
      "metadata": {
        "id": "c42b6143",
        "outputId": "77cc8454-182c-4215-d25c-892461c5d872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "type(ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03fee349",
      "metadata": {
        "id": "03fee349"
      },
      "source": [
        "## Defining the Neural network\n",
        "\n",
        "This is a 4 layer linear fully connected network. This is just an example\n",
        "Notice how the forward function is defined and how it uses the ReLu activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5decffcc",
      "metadata": {
        "id": "5decffcc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "555148b1",
      "metadata": {
        "id": "555148b1"
      },
      "outputs": [],
      "source": [
        "class Neural_net(nn.Module):    # this class inherits from nn.Module\n",
        "    def __init__(self):\n",
        "        super(Neural_net, self).__init__() #this calls the constructor of the parent class nn.Module\n",
        "\n",
        "        # define network layers\n",
        "        self.fc1 = nn.Linear(64, 20)   # nn.Linear is a class for linear layers (16,12) are the constructor arguments\n",
        "        self.fc2 = nn.Linear(20, 20)\n",
        "        self.fc3 = nn.Linear(20, 20)\n",
        "        self.fc4 = nn.Linear(20, 10)\n",
        "        torch.manual_seed(4)           # generating numbers changes the state of the random number generator.\n",
        "                                       # we thus have to set the seed back to 2\n",
        "\n",
        "        # Notice the RELU activation function below\n",
        "    def forward(self,x):\n",
        "        x = torch.relu(self.fc1(x))  #\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6f452c99",
      "metadata": {
        "id": "6f452c99"
      },
      "outputs": [],
      "source": [
        "net = Neural_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8644d743",
      "metadata": {
        "id": "8644d743"
      },
      "outputs": [],
      "source": [
        "def class_accuracy(output,y):\n",
        "    '''If np.argmax(out,axis=1)-y is non-zero, i.e. label y is 9 and prediction is 5 then\n",
        "       diff is incremented by 1, i.e. every time there is a mismatch between prediction and label.\n",
        "       The ratio diff/np.size(y) gives us the ratio of false predictions over the total number of datapoints.\n",
        "       One minus that gives the model accuracy.\n",
        "       '''\n",
        "    # we can't call numpy() on Tensors that requires grad. So, in order to compute diff (see below)\n",
        "    # we need to use tensor.detach().numpy()\n",
        "    output = output.cpu().detach().numpy()    # no need for the .cpu() here as we are working with cpu tensors\n",
        "    y      = y.cpu().detach().numpy()\n",
        "    diff   = np.count_nonzero(np.argmax(output,axis=1)-y) # np.argmax returns the index/indices of max value(s)\n",
        "                                                        # along specified axis\n",
        "    return (1-(diff/np.size(y)))*100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039aca31",
      "metadata": {
        "id": "039aca31"
      },
      "source": [
        "## Important Model and training parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1425302",
      "metadata": {
        "id": "f1425302"
      },
      "source": [
        "### The optimiser\n",
        "\n",
        "The optimiser module gives access to a large number of standard optimisers that try and help minimise the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8bd67598",
      "metadata": {
        "id": "8bd67598"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd2a611",
      "metadata": {
        "id": "0cd2a611"
      },
      "source": [
        "There are different options. Take a look at adam and sgd (stocastic gradient descent), both are very common choices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19cdd7ca",
      "metadata": {
        "id": "19cdd7ca"
      },
      "source": [
        "### The loss\n",
        "\n",
        "Typically for a classification problem one would use a cross entrop loss, the [torch documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) has some more details on this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff51795",
      "metadata": {
        "id": "8ff51795"
      },
      "source": [
        "Cross entropy loss is generally defined for two probability distributions $p$ and $q$ as:\n",
        "\n",
        "$$H(p,q) =  –\\sum_{x \\in \\mathcal{X}} p(x)  \\log(q(x))$$\n",
        "\n",
        "For binary classification\n",
        "\n",
        "$$\\mathrm{loss}=−(y \\log(p)+(1−y) \\log(1−p))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069b4de0",
      "metadata": {
        "id": "069b4de0"
      },
      "source": [
        "### The learning rate\n",
        "This is a parameter you set in your optimiser an you can play around with different values for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1bda2b40",
      "metadata": {
        "id": "1bda2b40",
        "outputId": "435358d1-88db-4c8e-cc11-c270d31a31ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Neural_net(\n",
              "  (fc1): Linear(in_features=64, out_features=20, bias=True)\n",
              "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (fc3): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (fc4): Linear(in_features=20, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6aedfb08",
      "metadata": {
        "id": "6aedfb08",
        "outputId": "7625a2c9-d674-48c3-bb5e-168544af46e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0/100, accuracy train 9.60 %, loss train, 2.34363, accuracy test 7.69 %, loss test, 2.30435\n",
            "epoch 1/100, accuracy train 9.60 %, loss train, 2.25952, accuracy test 15.38 %, loss test, 2.25562\n",
            "epoch 2/100, accuracy train 13.60 %, loss train, 2.21319, accuracy test 22.12 %, loss test, 2.21144\n",
            "epoch 3/100, accuracy train 21.60 %, loss train, 2.16640, accuracy test 29.81 %, loss test, 2.15994\n",
            "epoch 4/100, accuracy train 26.40 %, loss train, 2.11028, accuracy test 28.85 %, loss test, 2.09959\n",
            "epoch 5/100, accuracy train 31.20 %, loss train, 2.04803, accuracy test 35.58 %, loss test, 2.03633\n",
            "epoch 6/100, accuracy train 35.20 %, loss train, 1.98126, accuracy test 37.50 %, loss test, 1.97373\n",
            "epoch 7/100, accuracy train 33.60 %, loss train, 1.91527, accuracy test 42.31 %, loss test, 1.90679\n",
            "epoch 8/100, accuracy train 35.20 %, loss train, 1.84545, accuracy test 44.23 %, loss test, 1.83510\n",
            "epoch 9/100, accuracy train 41.60 %, loss train, 1.77001, accuracy test 51.92 %, loss test, 1.75612\n",
            "epoch 10/100, accuracy train 46.40 %, loss train, 1.69466, accuracy test 53.85 %, loss test, 1.66555\n",
            "epoch 11/100, accuracy train 44.00 %, loss train, 1.61598, accuracy test 54.81 %, loss test, 1.56781\n",
            "epoch 12/100, accuracy train 46.40 %, loss train, 1.53738, accuracy test 56.73 %, loss test, 1.47498\n",
            "epoch 13/100, accuracy train 48.00 %, loss train, 1.45886, accuracy test 55.77 %, loss test, 1.39708\n",
            "epoch 14/100, accuracy train 49.60 %, loss train, 1.38460, accuracy test 51.92 %, loss test, 1.33155\n",
            "epoch 15/100, accuracy train 53.60 %, loss train, 1.31609, accuracy test 55.77 %, loss test, 1.25247\n",
            "epoch 16/100, accuracy train 56.00 %, loss train, 1.24207, accuracy test 58.65 %, loss test, 1.16481\n",
            "epoch 17/100, accuracy train 60.00 %, loss train, 1.16702, accuracy test 58.65 %, loss test, 1.08295\n",
            "epoch 18/100, accuracy train 65.60 %, loss train, 1.09289, accuracy test 60.58 %, loss test, 1.00893\n",
            "epoch 19/100, accuracy train 67.20 %, loss train, 1.00792, accuracy test 69.23 %, loss test, 0.94299\n",
            "epoch 20/100, accuracy train 69.60 %, loss train, 0.92573, accuracy test 77.88 %, loss test, 0.87692\n",
            "epoch 21/100, accuracy train 75.20 %, loss train, 0.84970, accuracy test 79.81 %, loss test, 0.80967\n",
            "epoch 22/100, accuracy train 78.40 %, loss train, 0.77293, accuracy test 80.77 %, loss test, 0.75528\n",
            "epoch 23/100, accuracy train 85.60 %, loss train, 0.70513, accuracy test 84.62 %, loss test, 0.71093\n",
            "epoch 24/100, accuracy train 87.20 %, loss train, 0.63997, accuracy test 83.65 %, loss test, 0.66205\n",
            "epoch 25/100, accuracy train 87.20 %, loss train, 0.57183, accuracy test 82.69 %, loss test, 0.60814\n",
            "epoch 26/100, accuracy train 88.80 %, loss train, 0.50600, accuracy test 81.73 %, loss test, 0.56540\n",
            "epoch 27/100, accuracy train 88.80 %, loss train, 0.45046, accuracy test 82.69 %, loss test, 0.53732\n",
            "epoch 28/100, accuracy train 89.60 %, loss train, 0.40151, accuracy test 85.58 %, loss test, 0.51212\n",
            "epoch 29/100, accuracy train 90.40 %, loss train, 0.35614, accuracy test 84.62 %, loss test, 0.48686\n",
            "epoch 30/100, accuracy train 91.20 %, loss train, 0.31418, accuracy test 85.58 %, loss test, 0.46197\n",
            "epoch 31/100, accuracy train 92.80 %, loss train, 0.27730, accuracy test 84.62 %, loss test, 0.43595\n",
            "epoch 32/100, accuracy train 95.20 %, loss train, 0.24186, accuracy test 85.58 %, loss test, 0.41942\n",
            "epoch 33/100, accuracy train 96.80 %, loss train, 0.20896, accuracy test 85.58 %, loss test, 0.40002\n",
            "epoch 34/100, accuracy train 97.60 %, loss train, 0.18006, accuracy test 86.54 %, loss test, 0.37423\n",
            "epoch 35/100, accuracy train 98.40 %, loss train, 0.15508, accuracy test 86.54 %, loss test, 0.35686\n",
            "epoch 36/100, accuracy train 98.40 %, loss train, 0.13194, accuracy test 85.58 %, loss test, 0.35481\n",
            "epoch 37/100, accuracy train 98.40 %, loss train, 0.10980, accuracy test 84.62 %, loss test, 0.36758\n",
            "epoch 38/100, accuracy train 99.20 %, loss train, 0.09278, accuracy test 84.62 %, loss test, 0.38187\n",
            "epoch 39/100, accuracy train 99.20 %, loss train, 0.07976, accuracy test 85.58 %, loss test, 0.38640\n",
            "epoch 40/100, accuracy train 99.20 %, loss train, 0.06827, accuracy test 87.50 %, loss test, 0.37682\n",
            "epoch 41/100, accuracy train 99.20 %, loss train, 0.05799, accuracy test 87.50 %, loss test, 0.36309\n",
            "epoch 42/100, accuracy train 100.00 %, loss train, 0.04896, accuracy test 87.50 %, loss test, 0.36376\n",
            "epoch 43/100, accuracy train 100.00 %, loss train, 0.04164, accuracy test 84.62 %, loss test, 0.37646\n",
            "epoch 44/100, accuracy train 100.00 %, loss train, 0.03594, accuracy test 85.58 %, loss test, 0.39203\n",
            "epoch 45/100, accuracy train 100.00 %, loss train, 0.03066, accuracy test 86.54 %, loss test, 0.40312\n",
            "epoch 46/100, accuracy train 100.00 %, loss train, 0.02570, accuracy test 85.58 %, loss test, 0.41505\n",
            "epoch 47/100, accuracy train 100.00 %, loss train, 0.02157, accuracy test 83.65 %, loss test, 0.43163\n",
            "epoch 48/100, accuracy train 100.00 %, loss train, 0.01835, accuracy test 83.65 %, loss test, 0.45757\n",
            "epoch 49/100, accuracy train 100.00 %, loss train, 0.01576, accuracy test 81.73 %, loss test, 0.48827\n",
            "epoch 50/100, accuracy train 100.00 %, loss train, 0.01365, accuracy test 82.69 %, loss test, 0.51649\n",
            "epoch 51/100, accuracy train 100.00 %, loss train, 0.01189, accuracy test 82.69 %, loss test, 0.53837\n",
            "epoch 52/100, accuracy train 100.00 %, loss train, 0.01039, accuracy test 82.69 %, loss test, 0.55457\n",
            "epoch 53/100, accuracy train 100.00 %, loss train, 0.00905, accuracy test 83.65 %, loss test, 0.56771\n",
            "epoch 54/100, accuracy train 100.00 %, loss train, 0.00791, accuracy test 83.65 %, loss test, 0.57955\n",
            "epoch 55/100, accuracy train 100.00 %, loss train, 0.00698, accuracy test 82.69 %, loss test, 0.59401\n",
            "epoch 56/100, accuracy train 100.00 %, loss train, 0.00624, accuracy test 83.65 %, loss test, 0.61189\n",
            "epoch 57/100, accuracy train 100.00 %, loss train, 0.00562, accuracy test 83.65 %, loss test, 0.63198\n",
            "epoch 58/100, accuracy train 100.00 %, loss train, 0.00508, accuracy test 83.65 %, loss test, 0.65205\n",
            "epoch 59/100, accuracy train 100.00 %, loss train, 0.00459, accuracy test 82.69 %, loss test, 0.66910\n",
            "epoch 60/100, accuracy train 100.00 %, loss train, 0.00416, accuracy test 82.69 %, loss test, 0.68200\n",
            "epoch 61/100, accuracy train 100.00 %, loss train, 0.00378, accuracy test 82.69 %, loss test, 0.69027\n",
            "epoch 62/100, accuracy train 100.00 %, loss train, 0.00345, accuracy test 82.69 %, loss test, 0.69424\n",
            "epoch 63/100, accuracy train 100.00 %, loss train, 0.00317, accuracy test 82.69 %, loss test, 0.69491\n",
            "epoch 64/100, accuracy train 100.00 %, loss train, 0.00293, accuracy test 82.69 %, loss test, 0.69376\n",
            "epoch 65/100, accuracy train 100.00 %, loss train, 0.00272, accuracy test 82.69 %, loss test, 0.69269\n",
            "epoch 66/100, accuracy train 100.00 %, loss train, 0.00254, accuracy test 82.69 %, loss test, 0.69298\n",
            "epoch 67/100, accuracy train 100.00 %, loss train, 0.00238, accuracy test 82.69 %, loss test, 0.69535\n",
            "epoch 68/100, accuracy train 100.00 %, loss train, 0.00224, accuracy test 82.69 %, loss test, 0.69992\n",
            "epoch 69/100, accuracy train 100.00 %, loss train, 0.00211, accuracy test 82.69 %, loss test, 0.70655\n",
            "epoch 70/100, accuracy train 100.00 %, loss train, 0.00199, accuracy test 82.69 %, loss test, 0.71450\n",
            "epoch 71/100, accuracy train 100.00 %, loss train, 0.00188, accuracy test 82.69 %, loss test, 0.72270\n",
            "epoch 72/100, accuracy train 100.00 %, loss train, 0.00179, accuracy test 82.69 %, loss test, 0.73016\n",
            "epoch 73/100, accuracy train 100.00 %, loss train, 0.00170, accuracy test 82.69 %, loss test, 0.73630\n",
            "epoch 74/100, accuracy train 100.00 %, loss train, 0.00163, accuracy test 82.69 %, loss test, 0.74046\n",
            "epoch 75/100, accuracy train 100.00 %, loss train, 0.00156, accuracy test 82.69 %, loss test, 0.74215\n",
            "epoch 76/100, accuracy train 100.00 %, loss train, 0.00150, accuracy test 82.69 %, loss test, 0.74154\n",
            "epoch 77/100, accuracy train 100.00 %, loss train, 0.00144, accuracy test 82.69 %, loss test, 0.73888\n",
            "epoch 78/100, accuracy train 100.00 %, loss train, 0.00139, accuracy test 82.69 %, loss test, 0.73472\n",
            "epoch 79/100, accuracy train 100.00 %, loss train, 0.00134, accuracy test 82.69 %, loss test, 0.72976\n",
            "epoch 80/100, accuracy train 100.00 %, loss train, 0.00129, accuracy test 82.69 %, loss test, 0.72467\n",
            "epoch 81/100, accuracy train 100.00 %, loss train, 0.00125, accuracy test 82.69 %, loss test, 0.71997\n",
            "epoch 82/100, accuracy train 100.00 %, loss train, 0.00121, accuracy test 82.69 %, loss test, 0.71609\n",
            "epoch 83/100, accuracy train 100.00 %, loss train, 0.00117, accuracy test 82.69 %, loss test, 0.71331\n",
            "epoch 84/100, accuracy train 100.00 %, loss train, 0.00114, accuracy test 82.69 %, loss test, 0.71171\n",
            "epoch 85/100, accuracy train 100.00 %, loss train, 0.00111, accuracy test 84.62 %, loss test, 0.71124\n",
            "epoch 86/100, accuracy train 100.00 %, loss train, 0.00108, accuracy test 84.62 %, loss test, 0.71191\n",
            "epoch 87/100, accuracy train 100.00 %, loss train, 0.00105, accuracy test 84.62 %, loss test, 0.71349\n",
            "epoch 88/100, accuracy train 100.00 %, loss train, 0.00103, accuracy test 84.62 %, loss test, 0.71576\n",
            "epoch 89/100, accuracy train 100.00 %, loss train, 0.00100, accuracy test 84.62 %, loss test, 0.71840\n",
            "epoch 90/100, accuracy train 100.00 %, loss train, 0.00098, accuracy test 84.62 %, loss test, 0.72114\n",
            "epoch 91/100, accuracy train 100.00 %, loss train, 0.00096, accuracy test 84.62 %, loss test, 0.72379\n",
            "epoch 92/100, accuracy train 100.00 %, loss train, 0.00094, accuracy test 84.62 %, loss test, 0.72622\n",
            "epoch 93/100, accuracy train 100.00 %, loss train, 0.00092, accuracy test 84.62 %, loss test, 0.72833\n",
            "epoch 94/100, accuracy train 100.00 %, loss train, 0.00090, accuracy test 83.65 %, loss test, 0.73008\n",
            "epoch 95/100, accuracy train 100.00 %, loss train, 0.00088, accuracy test 83.65 %, loss test, 0.73146\n",
            "epoch 96/100, accuracy train 100.00 %, loss train, 0.00087, accuracy test 83.65 %, loss test, 0.73240\n",
            "epoch 97/100, accuracy train 100.00 %, loss train, 0.00085, accuracy test 83.65 %, loss test, 0.73296\n",
            "epoch 98/100, accuracy train 100.00 %, loss train, 0.00084, accuracy test 84.62 %, loss test, 0.73328\n",
            "epoch 99/100, accuracy train 100.00 %, loss train, 0.00082, accuracy test 84.62 %, loss test, 0.73343\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "# The criterion we want to optimse\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the optimiser\n",
        "# lr is the learnig rate\n",
        "optimizer = optim.Adam(net.parameters(), lr=5*10**-3)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i,data in enumerate(flat_dataloader_train):\n",
        "        # Load in the training datapoints\n",
        "        x=data[0]\n",
        "        y=data[1]\n",
        "        optimizer.zero_grad()\n",
        "        # The output of the current net\n",
        "        output = net(x)\n",
        "        loss = criterion(output,y)\n",
        "        loss.backward()\n",
        "        # Optimising one more step\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0 and i==0:\n",
        "            # test the accuracy\n",
        "            acc        = class_accuracy(output,y)\n",
        "            outputtest = net(xtest)\n",
        "            loss_test  = criterion(outputtest,ytest)\n",
        "            acc_test   = class_accuracy(outputtest,ytest)\n",
        "            print(f'epoch {epoch}/{num_epochs}, accuracy train {acc:.2f} %, loss train, {loss.item():.5f}, accuracy test {acc_test:.2f} %, loss test, {loss_test.item():.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de2bc56",
      "metadata": {
        "id": "6de2bc56"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Task:</b> Train the neural network using 100 epochs and plot the quantities printed to screen.  </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "116f7483",
      "metadata": {
        "id": "116f7483",
        "outputId": "74b01684-8263-4124-f1fb-aa39d788d157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.274426731350692\n",
            "epoch 0/100, accuracy train 11.20 %, loss train, 2.32492, accuracy test 19.23 %, loss test, 2.27443\n",
            "2.2238219707642313\n",
            "epoch 1/100, accuracy train 16.80 %, loss train, 2.26568, accuracy test 27.88 %, loss test, 2.22382\n",
            "2.1710896959188983\n",
            "epoch 2/100, accuracy train 21.60 %, loss train, 2.22211, accuracy test 29.81 %, loss test, 2.17109\n",
            "2.1138750963748407\n",
            "epoch 3/100, accuracy train 24.00 %, loss train, 2.17862, accuracy test 28.85 %, loss test, 2.11388\n",
            "2.0485490058055587\n",
            "epoch 4/100, accuracy train 23.20 %, loss train, 2.12721, accuracy test 29.81 %, loss test, 2.04855\n",
            "1.9756477731361703\n",
            "epoch 5/100, accuracy train 24.80 %, loss train, 2.06885, accuracy test 33.65 %, loss test, 1.97565\n",
            "1.895641984193232\n",
            "epoch 6/100, accuracy train 25.60 %, loss train, 2.00273, accuracy test 35.58 %, loss test, 1.89564\n",
            "1.8102789723026371\n",
            "epoch 7/100, accuracy train 27.20 %, loss train, 1.93173, accuracy test 37.50 %, loss test, 1.81028\n",
            "1.7214980465998908\n",
            "epoch 8/100, accuracy train 28.80 %, loss train, 1.85476, accuracy test 38.46 %, loss test, 1.72150\n",
            "1.633402092198963\n",
            "epoch 9/100, accuracy train 34.40 %, loss train, 1.77708, accuracy test 43.27 %, loss test, 1.63340\n",
            "1.5452550635823963\n",
            "epoch 10/100, accuracy train 40.80 %, loss train, 1.69543, accuracy test 50.96 %, loss test, 1.54526\n",
            "1.4613817245764573\n",
            "epoch 11/100, accuracy train 48.00 %, loss train, 1.60876, accuracy test 58.65 %, loss test, 1.46138\n",
            "1.3863646000379137\n",
            "epoch 12/100, accuracy train 51.20 %, loss train, 1.52213, accuracy test 61.54 %, loss test, 1.38636\n",
            "1.317239617813007\n",
            "epoch 13/100, accuracy train 56.80 %, loss train, 1.43253, accuracy test 65.38 %, loss test, 1.31724\n",
            "1.2370208325509515\n",
            "epoch 14/100, accuracy train 63.20 %, loss train, 1.33799, accuracy test 68.27 %, loss test, 1.23702\n",
            "1.1446363558850579\n",
            "epoch 15/100, accuracy train 66.40 %, loss train, 1.23608, accuracy test 72.12 %, loss test, 1.14464\n",
            "1.0480022583331312\n",
            "epoch 16/100, accuracy train 74.40 %, loss train, 1.12829, accuracy test 73.08 %, loss test, 1.04800\n",
            "0.9559273661804629\n",
            "epoch 17/100, accuracy train 77.60 %, loss train, 1.01825, accuracy test 76.92 %, loss test, 0.95593\n",
            "0.867943569995519\n",
            "epoch 18/100, accuracy train 80.00 %, loss train, 0.90736, accuracy test 78.85 %, loss test, 0.86794\n",
            "0.7809534213997777\n",
            "epoch 19/100, accuracy train 80.00 %, loss train, 0.79981, accuracy test 81.73 %, loss test, 0.78095\n",
            "0.7010121226195845\n",
            "epoch 20/100, accuracy train 83.20 %, loss train, 0.69859, accuracy test 83.65 %, loss test, 0.70101\n",
            "0.6412771394372359\n",
            "epoch 21/100, accuracy train 85.60 %, loss train, 0.60405, accuracy test 84.62 %, loss test, 0.64128\n",
            "0.6043770199072174\n",
            "epoch 22/100, accuracy train 88.80 %, loss train, 0.51334, accuracy test 83.65 %, loss test, 0.60438\n",
            "0.5550930312780687\n",
            "epoch 23/100, accuracy train 92.00 %, loss train, 0.42954, accuracy test 84.62 %, loss test, 0.55509\n",
            "0.5042899777566655\n",
            "epoch 24/100, accuracy train 92.80 %, loss train, 0.35532, accuracy test 84.62 %, loss test, 0.50429\n",
            "0.48333127134326415\n",
            "epoch 25/100, accuracy train 95.20 %, loss train, 0.29090, accuracy test 84.62 %, loss test, 0.48333\n",
            "0.45951516262413367\n",
            "epoch 26/100, accuracy train 96.00 %, loss train, 0.23948, accuracy test 84.62 %, loss test, 0.45952\n",
            "0.44317887874690687\n",
            "epoch 27/100, accuracy train 96.80 %, loss train, 0.19722, accuracy test 84.62 %, loss test, 0.44318\n",
            "0.4513483884996781\n",
            "epoch 28/100, accuracy train 96.80 %, loss train, 0.16123, accuracy test 84.62 %, loss test, 0.45135\n",
            "0.44573601965803056\n",
            "epoch 29/100, accuracy train 98.40 %, loss train, 0.12981, accuracy test 85.58 %, loss test, 0.44574\n",
            "0.4386246801385142\n",
            "epoch 30/100, accuracy train 100.00 %, loss train, 0.10295, accuracy test 86.54 %, loss test, 0.43862\n",
            "0.4373297877794222\n",
            "epoch 31/100, accuracy train 100.00 %, loss train, 0.08054, accuracy test 86.54 %, loss test, 0.43733\n",
            "0.4383397498794548\n",
            "epoch 32/100, accuracy train 100.00 %, loss train, 0.06250, accuracy test 86.54 %, loss test, 0.43834\n",
            "0.43964463184057906\n",
            "epoch 33/100, accuracy train 100.00 %, loss train, 0.04846, accuracy test 88.46 %, loss test, 0.43964\n",
            "0.44928244952822183\n",
            "epoch 34/100, accuracy train 100.00 %, loss train, 0.03741, accuracy test 88.46 %, loss test, 0.44928\n",
            "0.46782504169932193\n",
            "epoch 35/100, accuracy train 100.00 %, loss train, 0.02872, accuracy test 88.46 %, loss test, 0.46783\n",
            "0.4860157691307249\n",
            "epoch 36/100, accuracy train 100.00 %, loss train, 0.02209, accuracy test 88.46 %, loss test, 0.48602\n",
            "0.4928206985148715\n",
            "epoch 37/100, accuracy train 100.00 %, loss train, 0.01713, accuracy test 88.46 %, loss test, 0.49282\n",
            "0.49351394101424717\n",
            "epoch 38/100, accuracy train 100.00 %, loss train, 0.01324, accuracy test 89.42 %, loss test, 0.49351\n",
            "0.496128740484683\n",
            "epoch 39/100, accuracy train 100.00 %, loss train, 0.01030, accuracy test 89.42 %, loss test, 0.49613\n",
            "0.5026093804463732\n",
            "epoch 40/100, accuracy train 100.00 %, loss train, 0.00810, accuracy test 90.38 %, loss test, 0.50261\n",
            "0.5115137218891881\n",
            "epoch 41/100, accuracy train 100.00 %, loss train, 0.00647, accuracy test 91.35 %, loss test, 0.51151\n",
            "0.521124243413583\n",
            "epoch 42/100, accuracy train 100.00 %, loss train, 0.00524, accuracy test 91.35 %, loss test, 0.52112\n",
            "0.5294043688443849\n",
            "epoch 43/100, accuracy train 100.00 %, loss train, 0.00429, accuracy test 91.35 %, loss test, 0.52940\n",
            "0.5372301961337718\n",
            "epoch 44/100, accuracy train 100.00 %, loss train, 0.00355, accuracy test 91.35 %, loss test, 0.53723\n",
            "0.5451393549362419\n",
            "epoch 45/100, accuracy train 100.00 %, loss train, 0.00298, accuracy test 91.35 %, loss test, 0.54514\n",
            "0.5537591122521291\n",
            "epoch 46/100, accuracy train 100.00 %, loss train, 0.00252, accuracy test 91.35 %, loss test, 0.55376\n",
            "0.5634222660978744\n",
            "epoch 47/100, accuracy train 100.00 %, loss train, 0.00216, accuracy test 91.35 %, loss test, 0.56342\n",
            "0.5738097213244766\n",
            "epoch 48/100, accuracy train 100.00 %, loss train, 0.00186, accuracy test 91.35 %, loss test, 0.57381\n",
            "0.584425457792304\n",
            "epoch 49/100, accuracy train 100.00 %, loss train, 0.00162, accuracy test 91.35 %, loss test, 0.58443\n",
            "0.5947446631033728\n",
            "epoch 50/100, accuracy train 100.00 %, loss train, 0.00143, accuracy test 91.35 %, loss test, 0.59474\n",
            "0.6043097113134617\n",
            "epoch 51/100, accuracy train 100.00 %, loss train, 0.00128, accuracy test 91.35 %, loss test, 0.60431\n",
            "0.6128211699639868\n",
            "epoch 52/100, accuracy train 100.00 %, loss train, 0.00115, accuracy test 90.38 %, loss test, 0.61282\n",
            "0.6201858447472056\n",
            "epoch 53/100, accuracy train 100.00 %, loss train, 0.00104, accuracy test 90.38 %, loss test, 0.62019\n",
            "0.6264345958266611\n",
            "epoch 54/100, accuracy train 100.00 %, loss train, 0.00095, accuracy test 90.38 %, loss test, 0.62643\n",
            "0.6317070313043976\n",
            "epoch 55/100, accuracy train 100.00 %, loss train, 0.00087, accuracy test 90.38 %, loss test, 0.63171\n",
            "0.6361413864899517\n",
            "epoch 56/100, accuracy train 100.00 %, loss train, 0.00081, accuracy test 90.38 %, loss test, 0.63614\n",
            "0.6398984823408935\n",
            "epoch 57/100, accuracy train 100.00 %, loss train, 0.00076, accuracy test 90.38 %, loss test, 0.63990\n",
            "0.643119252048235\n",
            "epoch 58/100, accuracy train 100.00 %, loss train, 0.00071, accuracy test 90.38 %, loss test, 0.64312\n",
            "0.6459533939594357\n",
            "epoch 59/100, accuracy train 100.00 %, loss train, 0.00067, accuracy test 90.38 %, loss test, 0.64595\n",
            "0.6485272163513123\n",
            "epoch 60/100, accuracy train 100.00 %, loss train, 0.00063, accuracy test 90.38 %, loss test, 0.64853\n",
            "0.6509350336975525\n",
            "epoch 61/100, accuracy train 100.00 %, loss train, 0.00059, accuracy test 90.38 %, loss test, 0.65094\n",
            "0.653235966228957\n",
            "epoch 62/100, accuracy train 100.00 %, loss train, 0.00056, accuracy test 90.38 %, loss test, 0.65324\n",
            "0.6554377452642333\n",
            "epoch 63/100, accuracy train 100.00 %, loss train, 0.00054, accuracy test 90.38 %, loss test, 0.65544\n",
            "0.657557777213815\n",
            "epoch 64/100, accuracy train 100.00 %, loss train, 0.00051, accuracy test 90.38 %, loss test, 0.65756\n",
            "0.6595925609986696\n",
            "epoch 65/100, accuracy train 100.00 %, loss train, 0.00049, accuracy test 90.38 %, loss test, 0.65959\n",
            "0.6614966726885007\n",
            "epoch 66/100, accuracy train 100.00 %, loss train, 0.00047, accuracy test 90.38 %, loss test, 0.66150\n",
            "0.6633025503138761\n",
            "epoch 67/100, accuracy train 100.00 %, loss train, 0.00045, accuracy test 90.38 %, loss test, 0.66330\n",
            "0.6650092145177412\n",
            "epoch 68/100, accuracy train 100.00 %, loss train, 0.00044, accuracy test 90.38 %, loss test, 0.66501\n",
            "0.6665971493095817\n",
            "epoch 69/100, accuracy train 100.00 %, loss train, 0.00042, accuracy test 90.38 %, loss test, 0.66660\n",
            "0.6680279080763044\n",
            "epoch 70/100, accuracy train 100.00 %, loss train, 0.00041, accuracy test 90.38 %, loss test, 0.66803\n",
            "0.6693283722798314\n",
            "epoch 71/100, accuracy train 100.00 %, loss train, 0.00039, accuracy test 90.38 %, loss test, 0.66933\n",
            "0.6704992579124406\n",
            "epoch 72/100, accuracy train 100.00 %, loss train, 0.00038, accuracy test 90.38 %, loss test, 0.67050\n",
            "0.6715422828402914\n",
            "epoch 73/100, accuracy train 100.00 %, loss train, 0.00037, accuracy test 90.38 %, loss test, 0.67154\n",
            "0.6724615865832834\n",
            "epoch 74/100, accuracy train 100.00 %, loss train, 0.00036, accuracy test 90.38 %, loss test, 0.67246\n",
            "0.6732714200578928\n",
            "epoch 75/100, accuracy train 100.00 %, loss train, 0.00035, accuracy test 90.38 %, loss test, 0.67327\n",
            "0.6739826821294147\n",
            "epoch 76/100, accuracy train 100.00 %, loss train, 0.00035, accuracy test 90.38 %, loss test, 0.67398\n",
            "0.6746095845873619\n",
            "epoch 77/100, accuracy train 100.00 %, loss train, 0.00034, accuracy test 90.38 %, loss test, 0.67461\n",
            "0.6751688332306449\n",
            "epoch 78/100, accuracy train 100.00 %, loss train, 0.00033, accuracy test 90.38 %, loss test, 0.67517\n",
            "0.6756780450655786\n",
            "epoch 79/100, accuracy train 100.00 %, loss train, 0.00032, accuracy test 90.38 %, loss test, 0.67568\n",
            "0.6761532643010587\n",
            "epoch 80/100, accuracy train 100.00 %, loss train, 0.00032, accuracy test 90.38 %, loss test, 0.67615\n",
            "0.6766097276383432\n",
            "epoch 81/100, accuracy train 100.00 %, loss train, 0.00031, accuracy test 90.38 %, loss test, 0.67661\n",
            "0.6770811756445446\n",
            "epoch 82/100, accuracy train 100.00 %, loss train, 0.00030, accuracy test 90.38 %, loss test, 0.67708\n",
            "0.6775731197206067\n",
            "epoch 83/100, accuracy train 100.00 %, loss train, 0.00030, accuracy test 90.38 %, loss test, 0.67757\n",
            "0.6780792276058455\n",
            "epoch 84/100, accuracy train 100.00 %, loss train, 0.00029, accuracy test 90.38 %, loss test, 0.67808\n",
            "0.6785962594002571\n",
            "epoch 85/100, accuracy train 100.00 %, loss train, 0.00029, accuracy test 90.38 %, loss test, 0.67860\n",
            "0.6791198093320683\n",
            "epoch 86/100, accuracy train 100.00 %, loss train, 0.00029, accuracy test 90.38 %, loss test, 0.67912\n",
            "0.6796438695885534\n",
            "epoch 87/100, accuracy train 100.00 %, loss train, 0.00028, accuracy test 90.38 %, loss test, 0.67964\n",
            "0.6801617347127842\n",
            "epoch 88/100, accuracy train 100.00 %, loss train, 0.00028, accuracy test 90.38 %, loss test, 0.68016\n",
            "0.6806496946696465\n",
            "epoch 89/100, accuracy train 100.00 %, loss train, 0.00027, accuracy test 90.38 %, loss test, 0.68065\n",
            "0.6811340532414379\n",
            "epoch 90/100, accuracy train 100.00 %, loss train, 0.00027, accuracy test 90.38 %, loss test, 0.68113\n",
            "0.6816075354291736\n",
            "epoch 91/100, accuracy train 100.00 %, loss train, 0.00027, accuracy test 90.38 %, loss test, 0.68161\n",
            "0.6820622554805869\n",
            "epoch 92/100, accuracy train 100.00 %, loss train, 0.00026, accuracy test 90.38 %, loss test, 0.68206\n",
            "0.6824903148752681\n",
            "epoch 93/100, accuracy train 100.00 %, loss train, 0.00026, accuracy test 90.38 %, loss test, 0.68249\n",
            "0.6828393564133338\n",
            "epoch 94/100, accuracy train 100.00 %, loss train, 0.00026, accuracy test 90.38 %, loss test, 0.68284\n",
            "0.6831058124727988\n",
            "epoch 95/100, accuracy train 100.00 %, loss train, 0.00025, accuracy test 90.38 %, loss test, 0.68311\n",
            "0.6832986653277234\n",
            "epoch 96/100, accuracy train 100.00 %, loss train, 0.00025, accuracy test 90.38 %, loss test, 0.68330\n",
            "0.6834276429501998\n",
            "epoch 97/100, accuracy train 100.00 %, loss train, 0.00025, accuracy test 90.38 %, loss test, 0.68343\n",
            "0.683504217107662\n",
            "epoch 98/100, accuracy train 100.00 %, loss train, 0.00024, accuracy test 90.38 %, loss test, 0.68350\n",
            "0.6835409107126705\n",
            "epoch 99/100, accuracy train 100.00 %, loss train, 0.00024, accuracy test 90.38 %, loss test, 0.68354\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Your solution here:\n",
        "\n",
        "net = Neural_net()\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# The criterion we want to optimse\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=5*10**-3)\n",
        "\n",
        "# List to collect training accuracy, test loss, and test accuracy at each epoch values\n",
        "info = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i,data in enumerate(flat_dataloader_train):\n",
        "        # Load in the training datapoints\n",
        "        x=data[0]\n",
        "        y=data[1]\n",
        "        optimizer.zero_grad()\n",
        "        # The output of the current net\n",
        "        output = net(x)\n",
        "        loss = criterion(output,y)\n",
        "        loss.backward()\n",
        "        # Optimising one more step\n",
        "        optimizer.step()\n",
        "\n",
        "        # i==0 -> ensures following are reported only once per epoch\n",
        "        # acc -> computest training accuracy\n",
        "        # loss_test converted to numpy float so easier to store -> computes test loss\n",
        "        # acc_test -> comp. test accuracy\n",
        "        if epoch % 1 == 0 and i==0:\n",
        "            # test the accuracy\n",
        "            acc        = class_accuracy(output,y)\n",
        "            outputtest = net(xtest)\n",
        "            loss_test  = criterion(outputtest,ytest)\n",
        "            acc_test   = class_accuracy(outputtest,ytest)\n",
        "            print(float(loss_test))\n",
        "\n",
        "            # Store the values -> all added to info list using append\n",
        "            info.append([acc, float(loss_test), acc_test])\n",
        "            print(f'epoch {epoch}/{num_epochs}, accuracy train {acc:.2f} %, loss train, {loss.item():.5f}, accuracy test {acc_test:.2f} %, loss test, {loss_test.item():.5f}')\n",
        "\n",
        "# convert list to numpy array for easier slicing & plotting\n",
        "info = np.array(info)\n",
        "\n",
        "# create linearly spaced array from 0 to num_epochs w/ same length as info -> one value per epoch stored ensures x-axis aligns w/ epoch n°\n",
        "epochs = np.linspace(0, num_epochs, len(info))\n",
        "\n",
        "plt.plot(epochs, info[:, 0], label='Training Accuracy')\n",
        "plt.plot(epochs, info[:, 1], label='Test Loss')\n",
        "plt.plot(epochs, info[:, 2], label='Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e23925b",
      "metadata": {
        "id": "9e23925b"
      },
      "source": [
        "<details>\n",
        "    <summary> <mark> Solution: </mark> </summary>\n",
        "\n",
        "```Python\n",
        "# Let's reinisalise the network\n",
        "net = Neural_net()\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# The criterion we want to optimse\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the optimiser\n",
        "# lr is the learnig rate\n",
        "optimizer = optim.Adam(net.parameters(), lr=5*10**-3)\n",
        "\n",
        "# list where we want to store the training information\n",
        "info = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i,data in enumerate(flat_dataloader_train):\n",
        "        # Load in the training datapoints\n",
        "        x=data[0]                 \n",
        "        y=data[1]   \n",
        "        optimizer.zero_grad()\n",
        "        # The output of the current net\n",
        "        output = net(x)\n",
        "        loss = criterion(output,y)\n",
        "        loss.backward()\n",
        "        # Optimising one more step\n",
        "        optimizer.step()\n",
        "                   \n",
        "        if epoch % 1 == 0 and i==0:\n",
        "            # test the accuracy\n",
        "            acc        = class_accuracy(output,y)\n",
        "            outputtest = net(xtest)\n",
        "            loss_test  = criterion(outputtest,ytest).detach().cpu().numpy()\n",
        "            acc_test   = class_accuracy(outputtest,ytest)\n",
        "            print(float(loss_test))\n",
        "            info.append([acc, float(loss_test),acc_test])\n",
        "            print(f'epoch {epoch}/{num_epochs}, accuracy train {acc:.2f} %, loss train, {loss.item():.5f}, accuracy test {acc_test:.2f} %, loss test, {loss_test.item():.5f}')\n",
        "info = np.array(info)\n",
        "epochs = np.linspace(0,num_epochs,len(info))\n",
        "plt.plot(epochs, info[:,0], label='accuracy')\n",
        "plt.plot(epochs, info[:,1], label='loss_test')\n",
        "plt.plot(epochs, info[:,2], label='accuracy test')\n",
        "plt.legend()\n",
        "```\n",
        "\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9549c91",
      "metadata": {
        "id": "c9549c91"
      },
      "source": [
        "## END\n",
        "------"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}